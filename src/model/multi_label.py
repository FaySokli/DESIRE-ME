# -*- coding: utf-8 -*-
"""Multi-Label.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1h03zetwWB6EeP8Phg3tI1s2sl50AqtdF

Required packages:\
pandas==1.4.0\
numpy==1.21.5\
scikit-learn==1.0.2\
tensorflow==2.7.0\
torch==1.10.2\
transformers==4.17.0.dev0\
datasets==1.18.3\
textstat==0.7.2 (if running the ML part)\
xgboost==1.5.2 (if running the ML part)
"""

# pip install fsspec==2023.6.0
# pip install torch==2.2.1
# pip install accelerate==0.26.1
# pip install textstat
# pip install datasets
# python3 -m pip install pandas
# python3 -m pip install -U scikit-learn
# python3 -m pip install tensorflow
# python3 -m pip install transformers

import pandas as pd
import numpy as np
from beir import util, LoggingHandler
from beir.datasets.data_loader import GenericDataLoader
import sys

data = pd.read_csv("/home/ubuntu/esokli/CognitiveComplexityMoE/Data/sample_full.csv")

data.fillna({'Remember': 0, 'Understand': 0, 'Apply': 0, 'Analyze': 0, 'Evaluate': 0, 'Create':0}, inplace=True)

LIWC_data = pd.read_csv("/home/ubuntu/esokli/CognitiveComplexityMoE/Data/LIWC2015 Results (Learning_outcome.csv).csv")
data = data.join(LIWC_data).drop(['A'], axis=1)

data.head()

labels = data[data.columns[1:7]].values.tolist()

data.columns[1:7]

from sklearn.ensemble import RandomForestClassifier

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.model_selection import cross_validate, GridSearchCV
from sklearn import metrics
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, cohen_kappa_score, f1_score

"""## BERT"""

import torch
import tensorflow as tf
from transformers import AutoTokenizer, AutoModel, TrainingArguments, Trainer, AutoModelForSequenceClassification, EarlyStoppingCallback
from transformers import TFBertPreTrainedModel, TFBertMainLayer, InputFeatures
from datasets import load_metric, list_metrics
from torch.utils.data import TensorDataset, DataLoader, SequentialSampler
import logging
from tqdm import tqdm
import json

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EncodeDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

######################################################
######################################################

train_x, test_x, train_y, test_y = train_test_split(data['Learning_outcome'].tolist(), labels, test_size=0.2, random_state=666)
train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=666)

class BertMultiLabelClassifier:
    def __init__(self, model_name='bert-base-uncased', checkpoint_path='/home/ubuntu/esokli/CognitiveComplexityMoE/Classifier/Checkpoints/checkpoint-530'):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, problem_type="multi_label_classification")
        if checkpoint_path:
            self.model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path, num_labels=6, problem_type="multi_label_classification")
        else:
            self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=6, problem_type="multi_label_classification")
        # self.model = AutoModelForSequenceClassification.from_pretrained('/home/ubuntu/esokli/CognitiveComplexityMoE/Classifier/Checkpoints/checkpoint-530', num_labels=6, problem_type="multi_label_classification")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self.model.to(self.device)

    def train(self, train_x, train_y, val_x, val_y):
        train_encoded = self.tokenizer(train_x, truncation=True, padding=True, max_length=100)
        val_encoded   = self.tokenizer(val_x, truncation=True, padding=True, max_length=100)
        test_encoded  = self.tokenizer(test_x, truncation=True, padding=True, max_length=100)

        train_set, val_set, test_set = EncodeDataset(train_encoded, train_y), EncodeDataset(val_encoded, val_y), EncodeDataset(test_encoded, test_y)
        
        training_args = TrainingArguments(
                output_dir='/home/ubuntu/esokli/CognitiveComplexityMoE/Classifier/Checkpoints',          # output directory
                overwrite_output_dir=True,
                num_train_epochs=3,              # total number of training epochs
                per_device_train_batch_size=64,  # batch size per device during training
                per_device_eval_batch_size=64,   # batch size for evaluation
                warmup_steps=5,                # number of warmup steps for learning rate scheduler
                weight_decay=0.05,               # strength of weight decay
                logging_dir='./logs',            # directory for storing logs
                logging_steps=10,
                evaluation_strategy="steps",
                save_strategy="steps",
                save_steps=10,
                load_best_model_at_end=True
            )
        
        trainer = Trainer(model=self.model, args=training_args, train_dataset=train_set, eval_dataset=val_set, callbacks=[EarlyStoppingCallback(early_stopping_patience=5)])
        trainer.train()
        logits = trainer.predict(test_set)
        logits.predictions.shape
        predicted = tf.keras.activations.sigmoid(logits.predictions)
        predicted.numpy()
        predicted_label = self.getClassResult(predicted)
        return predicted_label, logits

    def getClassResult(self, predicted):
        results = []
        for probs in predicted.numpy():
            result = []
            for prob in probs:
                if prob < 0.5:
                    result.append(0)
                else:
                    result.append(1)
            results.append(result)
        return results

    metric = load_metric("f1")
    def compute_metrics(self, eval_pred):
        logits, labels = eval_pred
        predictions = tf.keras.activations.sigmoid(logits)
        predicted = getClassResult(predictions)
        return metric.compute(predictions=predicted, references=labels, average="micro")

    # Function to predict labels using BERT
    def predict_labels(self, text_ids, texts, batch_size=16):
        preds = []
        for i in tqdm(range(0, len(texts), batch_size), desc="Predicting labels..."):
            batch_text_ids = text_ids[i:i+batch_size].tolist()
            batch_texts = texts[i:i+batch_size].tolist()
            inputs = self.tokenizer(batch_texts, return_tensors='pt', padding=True, truncation=True, max_length=512)
            inputs = {name: tensor.to(self.device) for name, tensor in inputs.items()}  # move inputs to device
            with torch.no_grad():
                outputs = self.model(**inputs)
            _, batch_preds_indices = torch.max(outputs.logits, dim=1)
            for j in range(len(batch_text_ids)):
                preds.append({"_id": batch_text_ids[j], "category": str(batch_preds_indices[j].item())})
        return preds

    def predict_logits(self, text_ids, texts, batch_size=16):
        text_id_to_logits = {}
        for i in tqdm(range(0, len(texts), batch_size), desc="Predicting logits..."):
            batch_texts = texts[i:i+batch_size]
            inputs = self.tokenizer(batch_texts.to_list(), return_tensors='pt', padding=True, truncation=True, max_length=512)
            inputs = {name: tensor.to(self.device) for name, tensor in inputs.items()}
            with torch.no_grad():
                outputs = self.model(**inputs).logits
            for j in range(outputs.shape[0]):
                text_id_to_logits[text_ids.iloc[i+j]] = outputs[j].cpu().tolist()
            
        return text_id_to_logits
######################################################
######################################################
train = False
if train:
    classifier = BertMultiLabelClassifier('bert-base-uncased', None)
    predicted_label, logits = classifier.train(train_x, train_y, val_x, val_y)
    count = 0
    for pred in predicted_label:
        if pred.count(1) > 1:
            count += 1
    count
    print(classification_report(test_y, predicted_label, output_dict=False, target_names=list(data.columns[1:7]), digits=3))

    # roc_auc_score(test_y, predicted_label.numpy(), average=None)

    accuracy_score(np.array(test_y), predicted_label)

    dl_result_df = pd.DataFrame(data=predicted_label, columns=data.columns[1:7])

else:
    classifier = BertMultiLabelClassifier('bert-base-uncased', '/home/ubuntu/esokli/CognitiveComplexityMoE/Classifier/Checkpoints/checkpoint-530')


# Load the datasets
# logger.info("Loading msmarco...")
# queries = pd.read_csv('/home/ubuntu/esokli/CognitiveComplexityMoE/Data/queries.csv')
# passages = pd.read_csv('/home/ubuntu/esokli/CognitiveComplexityMoE/Data/passages.csv')

# Filter the 'passages' dataframe
# logger.info("Filtering passages...")
# passages_filtered = passages[passages['passage_param'] == 'passage_text']

######################################################
######################################################
from datasets import load_dataset
dataset = load_dataset("json", data_files="nq/queries.jsonl")
nqq = dataset['train'].to_pandas()

# Predict labels for the 'queries' and 'passages' datasets
logger.info("Predicting logits for queries...")
# queries['label'] = classifier.predict_logits(queries['query_id'], queries['query'])
queries_to_logits = classifier.predict_logits(nqq['_id'].astype(str), nqq['text'])
queries_to_logits = {str(k): v for k, v in queries_to_logits.items()}
logger.info("Saving queries results...")
with open('/home/ubuntu/esokli/DESIRE-ME/nq/queries_to_logits_nq.json', 'w') as f:
    json.dump(queries_to_logits, f, indent=2)
# queries.to_csv('queries_with_logits.csv', index=False)

######################################################
######################################################

from datasets import load_dataset
dataset = load_dataset("json", data_files="/home/ubuntu/esokli/DESIRE-ME/nq/wiki_corpus.jsonl")
nq = dataset['train'].to_pandas()
nq = nq.drop(columns=['metadata'])
nq = nq.drop(columns=['category'])

logger.info("Predicting labels for docs...")
nq.reset_index(drop=True, inplace=True)
nq_docs_labelled = classifier.predict_labels(nq['_id'].astype(str), nq['text'])
logger.info("Saving docs results...")
with open('/home/ubuntu/esokli/DESIRE-ME/nq/wiki_corpus_nq.json', 'w') as f:
    json.dump(nq_docs_labelled, f, indent=2)

# Merge the predicted labels back into the original 'passages' DataFrame
logger.info("Merging predicted labels for passages...")
passages = passages.merge(passages_filtered[['label']], left_index=True, right_index=True, how='left')

logger.info("Saving passages results...")
passages.to_csv('passages_with_labels.csv', index=False)

logger.info("Done.")


# tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', problem_type="multi_label_classification")
# model = AutoModelForSequenceClassification.from_pretrained('/home/ubuntu/esokli/CognitiveComplexityMoE/Classifier/Checkpoints/checkpoint-520', num_labels=6, problem_type="multi_label_classification")
# model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=6, problem_type="multi_label_classification")
# device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# model = model.to(device)

# train_x, test_x, train_y, test_y = train_test_split(data['Learning_outcome'].tolist(), labels, test_size=0.2, random_state=666)
# train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, random_state=666)

# train_encoded = tokenizer(train_x, truncation=True, padding=True, max_length=100)
# val_encoded = tokenizer(val_x, truncation=True, padding=True, max_length=100)
# test_encoded = tokenizer(test_x, truncation=True, padding=True, max_length=100)

# train_set, val_set, test_set = EncodeDataset(train_encoded, train_y), EncodeDataset(val_encoded, val_y), EncodeDataset(test_encoded, test_y)

# training_args = TrainingArguments(
#         output_dir='/home/ubuntu/esokli/CognitiveComplexityMoE/Classifier/Checkpoints',          # output directory
#         overwrite_output_dir=True,
#         num_train_epochs=3,              # total number of training epochs
#         per_device_train_batch_size=64,  # batch size per device during training
#         per_device_eval_batch_size=64,   # batch size for evaluation
#         warmup_steps=5,                # number of warmup steps for learning rate scheduler
#         weight_decay=0.05,               # strength of weight decay
#         logging_dir='./logs',            # directory for storing logs
#         logging_steps=10,
#         evaluation_strategy="steps",
#         save_strategy="steps",
#         save_steps=10,
#         load_best_model_at_end=True
#     )

# def getClassResult(predicted):
#     results = []
#     for probs in predicted.numpy():
#         result = []
#         for prob in probs:
#             if prob < 0.5:
#                 result.append(0)
#             else:
#                 result.append(1)
#         results.append(result)
#     return results

# metric = load_metric("f1")
# def compute_metrics(eval_pred):
#     logits, labels = eval_pred
#     predictions = tf.keras.activations.sigmoid(logits)
#     predicted = getClassResult(predictions)
#     return metric.compute(predictions=predicted, references=labels, average="micro")

# trainer = Trainer(model=model, args=training_args, train_dataset=train_set, eval_dataset=val_set, callbacks=[EarlyStoppingCallback(early_stopping_patience=5)])

# trainer.train()

# logits = trainer.predict(test_set)

# logits.predictions.shape

# predicted = tf.keras.activations.sigmoid(logits.predictions)

# predicted.numpy()

# predicted_label = getClassResult(predicted)

# count = 0
# for pred in predicted_label:
#     if pred.count(1) > 1:
#         count += 1
# count

# print(classification_report(test_y, predicted_label, output_dict=False, target_names=list(data.columns[1:7]), digits=3))

# roc_auc_score(test_y, predicted.numpy(), average=None)

# accuracy_score(np.array(test_y), predicted_label)

# dl_result_df = pd.DataFrame(data=predicted_label, columns=data.columns[1:7])

# print(accuracy_score(ml_golden_df['Remember'].tolist(), dl_result_df['Remember'].tolist()))
# print(accuracy_score(ml_golden_df['Understand'].tolist(), dl_result_df['Understand'].tolist()))
# print(accuracy_score(ml_golden_df['Apply'].tolist(), dl_result_df['Apply'].tolist()))
# print(accuracy_score(ml_golden_df['Analyze'].tolist(), dl_result_df['Analyze'].tolist()))
# print(accuracy_score(ml_golden_df['Evaluate'].tolist(), dl_result_df['Evaluate'].tolist()))
# print(accuracy_score(ml_golden_df['Create'].tolist(), dl_result_df['Create'].tolist()))

# print(cohen_kappa_score(ml_golden_df['Remember'].tolist(), dl_result_df['Remember'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Understand'].tolist(), dl_result_df['Understand'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Apply'].tolist(), dl_result_df['Apply'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Analyze'].tolist(), dl_result_df['Analyze'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Evaluate'].tolist(), dl_result_df['Evaluate'].tolist()))
# print(cohen_kappa_score(ml_golden_df['Create'].tolist(), dl_result_df['Create'].tolist()))

# # Load the datasets
# logger.info("Loading datasets...")
# queries = pd.read_csv('/home/ubuntu/esokli/CognitiveComplexityMoE/Data/queries.csv')
# passages = pd.read_csv('/home/ubuntu/esokli/CognitiveComplexityMoE/Data/passages.csv')

# # Filter the 'passages' dataframe
# logger.info("Filtering passages...")
# passages_filtered = passages[passages['passage_param'] == 'passage_text']

# # Function to predict labels using BERT
# def predict_labels(texts, batch_size=16):
#     logger.info("Predicting labels...")
#     preds = []
#     for i in range(0, len(texts), batch_size):
#         logger.info(f"Processing batch {i//batch_size + 1}/{len(texts)//batch_size + 1}...")
#         batch_texts = texts[i:i+batch_size]
#         inputs = tokenizer(batch_texts.to_list(), return_tensors='pt', padding=True, truncation=True, max_length=512)
#         inputs = {name: tensor.to(device) for name, tensor in inputs.items()}  # move inputs to device
#         with torch.no_grad():
#             outputs = model(**inputs)
#         _, batch_preds = torch.max(outputs.logits, dim=1)
#         preds.extend(batch_preds.tolist())
#     return preds